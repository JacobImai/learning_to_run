DDPG:
    use_bn: False
    save_snapshot_every: 1000
    train_every: 1
    num_train: 1
    jump: False
    gamma: 0.99
    tau: 0.001
    batch_size: 128
    actor_l2: 0.000001
    actor_lr: 0.0001
    actor_l2_action: 0.000001
    critic_l2: 0.000001
    critic_lr: 0.0003
    merge_at_layer: 1
    theta: 0.005
    sigma_init: 0.02
    sigma_min: 0.002
    scale_min: 0.08
    total_episodes: 50000
    max_steps: 1000
    memory_warmup: 10000
    memory_capacity: 1000000
    annealing_steps: 4000000
    actor_hiddens: [128, 128, 64, 64]
    critic_hiddens: [128, 128, 64, 64]
    ob_processor: "2ndround"
    mirror_ob: True
    rs_weight: 0.0005
    include_limb_vel: True
    lrelu: 0.3
    reward_scale: 10.0
    use_ln: False
    max_obstacles: 10
    ob_dist_scale: 1.0
    fake_ob_pos: 3.0
    clear_vel: False
    use_swish: False
    num_samplers: 2
    clip_norm: 0.0
    queue_size: 10000
TRPO:
    batch_size: 5000
    n_envs: 16
    n_iters: 5000
    ob_processor: "bodyspeed"
    hidden_nonlinearity: "relu"
    action_nonlinearity: "tanh"
    policy_hiddens: [128, 128, 64, 64]
    baseline_hiddens: [128, 128, 64, 64]
    last_iter: -1
    discount: 0.99
    gae_lambda: 0.97
    step_size: 0.01
    use_linesearch: True
    kl_subsamp_ratio: 1.
    snapshot_saver: None
    jump: False

